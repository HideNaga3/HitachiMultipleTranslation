# よく発生するエラーと対処法

このドキュメントは、多言語PDF抽出プロジェクトで頻発するエラーとその対処法をまとめたものです。

作成日: 2025-10-24

---

## 1. UnicodeEncodeError: 'cp932' codec can't encode character

### エラー内容
```
UnicodeEncodeError: 'cp932' codec can't encode character '\u1780' in position 89: illegal multibyte sequence
```

### 発生原因
- Windows PowerShellのデフォルトエンコーディングがcp932（Shift-JIS）
- クメール語、タイ語、ミャンマー語などの多言語文字を含む文字列をprint()すると発生
- stdoutがcp932に設定されているため、Unicode文字を出力できない

### 対処法

#### 方法1: スクリプトの先頭でstdoutをUTF-8に強制変更（推奨）

```python
import sys
import io

# UTF-8出力を強制
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
```

このコードを**すべてのimport文の直後**に追加する。

#### 方法2: 出力をファイルにリダイレクト

```bash
python script.py > output.txt 2>&1
```

コンソール出力せず、ファイルに保存する。

#### 方法3: print文を使わず、ファイル出力のみにする

```python
# 出力をログファイルに書き込む
with open('output.log', 'w', encoding='utf-8') as f:
    f.write(f"結果: {result}\n")
```

### 適用例

**修正前:**
```python
import pandas as pd

df = pd.read_csv('カンボジア語.csv', encoding='utf-8-sig')
print(df.head())  # ← エラー発生
```

**修正後:**
```python
import pandas as pd
import sys
import io

# UTF-8出力を強制
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

df = pd.read_csv('カンボジア語.csv', encoding='utf-8-sig')
print(df.head())  # ← OK
```

---

## 2. TypeError: object of type 'TableFinder' has no len()

### エラー内容
```
TypeError: object of type 'TableFinder' has no len()
```

### 発生原因
- PyMuPDF (fitz) 1.26.5のAPI変更
- `page.find_tables()`が`TableFinder`オブジェクトを返す
- `TableFinder`に対して直接`len()`を使用するとエラー

### 対処法

#### 正しいAPI使用方法

```python
# ❌ 間違い
tables = page.find_tables()
if len(tables) > 0:  # ← エラー
    for table in tables:
        ...

# ✓ 正しい
table_finder = page.find_tables()
tables = table_finder.tables  # TableFinderからテーブルリストを取得
if len(tables) > 0:  # ← OK
    for table in tables:
        ...
```

### 適用例

**修正前:**
```python
import fitz

doc = fitz.open('document.pdf')
page = doc[0]
tables = page.find_tables()

if len(tables) > 0:  # ← エラー
    for table in tables:
        data = table.extract()
```

**修正後:**
```python
import fitz

doc = fitz.open('document.pdf')
page = doc[0]

# TableFinderからテーブルリストを取得
table_finder = page.find_tables()
tables = table_finder.tables

if len(tables) > 0:  # ← OK
    for table in tables:
        data = table.extract()
```

---

## 3. ValueError: columns passed, passed data had N columns

### エラー内容
```
ValueError: 9 columns passed, passed data had 15 columns
```

### 発生原因
- PDF表の列数がページや表ごとに異なる
- DataFrameの列名リストとデータの列数が一致しない
- PDF抽出時に行ごとに列数が変わることがある

### 対処法

#### 最大列数に揃えてからDataFrame作成

```python
# 各行の列数を確認して最大列数を取得
max_columns = max(len(row) for row in all_data)

# すべての行を最大列数に揃える
normalized_data = []
for row in all_data:
    if len(row) < max_columns:
        # 不足分を空文字列で埋める
        normalized_row = list(row) + [''] * (max_columns - len(row))
    else:
        normalized_row = list(row)
    normalized_data.append(normalized_row)

# 列名を生成
column_names = ['Page', 'Table'] + [f'Column_{i}' for i in range(max_columns - 2)]

# DataFrame作成
df = pd.DataFrame(normalized_data, columns=column_names)
```

### 適用例

**修正前:**
```python
all_data = []
for page in doc:
    tables = page.find_tables().tables
    for table in tables:
        rows = table.extract()[1:]  # ヘッダーを除く
        all_data.extend(rows)

# 列数が不揃いでエラー
df = pd.DataFrame(all_data, columns=['A', 'B', 'C'])  # ← エラー
```

**修正後:**
```python
all_data = []
for page in doc:
    tables = page.find_tables().tables
    for table in tables:
        rows = table.extract()[1:]
        all_data.extend(rows)

# 最大列数に揃える
max_columns = max(len(row) for row in all_data)
normalized_data = []
for row in all_data:
    if len(row) < max_columns:
        normalized_row = list(row) + [''] * (max_columns - len(row))
    else:
        normalized_row = list(row)
    normalized_data.append(normalized_row)

# 動的に列名を生成
column_names = [f'Column_{i}' for i in range(max_columns)]
df = pd.DataFrame(normalized_data, columns=column_names)
```

---

## 4. PowerShell出力の文字化け

### 問題内容
- PowerShellでスクリプトを実行すると日本語やクメール文字が文字化け
- エラーメッセージが読めない

### 対処法

#### 方法1: 出力をUTF-8ファイルにリダイレクト

```powershell
.venv\Scripts\python.exe script.py > output.txt 2>&1
```

その後、UTF-8対応エディタ（VS Code、Notepad++など）で開く。

#### 方法2: Windows Terminalを使用

PowerShellの代わりにWindows Terminalを使用すると、UTF-8がデフォルトで有効。

#### 方法3: PowerShellのエンコーディングを変更

```powershell
[Console]::OutputEncoding = [System.Text.Encoding]::UTF8
chcp 65001
```

ただし、これは一時的な変更で、新しいセッションでは元に戻る。

---

## 5. CIDコード問題（pdfplumber特有）

### 問題内容
```
出力: (cid:688)បព័ន(cid:640)ែខ(cid:717)សុវត(cid:630)ិ(cid:671)ព
期待: របព័ន្ធែខ្សុវត្ថិភាព
```

### 発生原因
- pdfplumberがクメール語、タイ語のダイアクリティカルマーク（母音記号）を認識できない
- 特殊文字をCIDコード（Character ID）として出力してしまう

### 対処法

#### PyMuPDFを使用（推奨）

```python
import fitz  # PyMuPDF

doc = fitz.open('カンボジア語.pdf')
page = doc[0]

# PyMuPDFはCIDコードを生成しない
table_finder = page.find_tables()
tables = table_finder.tables

for table in tables:
    data = table.extract()
    # クメール文字が正しく抽出される
```

#### 検証方法

```python
import re

# CIDコードをチェック
cid_pattern = r'\(cid:\d+\)'
text = "របព័ន្ធែខ្សុវត្ថិភាព"

if re.search(cid_pattern, text):
    print("CIDコードあり")
else:
    print("CIDコードなし - OK")
```

---

## 6. ファイルパスのエラー（日本語ファイル名）

### 問題内容
- 日本語を含むファイルパスでエラー
- PowerShellでのパス指定がうまくいかない

### 対処法

#### pathlibを使用（推奨）

```python
from pathlib import Path

# ✓ 推奨: pathlibを使用
pdf_path = Path('建設関連PDF') / '【全課統合版】カンボジア語_げんばのことば_建設関連職種.pdf'

# ❌ 避ける: 文字列結合
pdf_path = '建設関連PDF\\' + 'カンボジア語.pdf'
```

#### PowerShellでのパス指定

```powershell
# ダブルクォートで囲む
python script.py "C:\python_script\test_space\カンボジア語.csv"

# または、相対パスを使用
python script.py "output\カンボジア語.csv"
```

---

## 7. pandas read_csv エンコーディングエラー

### エラー内容
```
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0
```

### 発生原因
- CSVファイルのエンコーディングが想定と異なる
- BOM（Byte Order Mark）付きUTF-8の可能性

### 対処法

```python
import pandas as pd

# UTF-8 BOMで読み込み（推奨）
df = pd.read_csv('file.csv', encoding='utf-8-sig')

# エンコーディングが不明な場合
# 方法1: chardetで自動検出
import chardet
with open('file.csv', 'rb') as f:
    result = chardet.detect(f.read())
    encoding = result['encoding']
df = pd.read_csv('file.csv', encoding=encoding)

# 方法2: 複数のエンコーディングを試す
encodings = ['utf-8-sig', 'utf-8', 'cp932', 'shift-jis']
for enc in encodings:
    try:
        df = pd.read_csv('file.csv', encoding=enc)
        print(f"成功: {enc}")
        break
    except UnicodeDecodeError:
        continue
```

---

## ベストプラクティス

### 1. スクリプトテンプレート

多言語データを扱うPythonスクリプトの基本テンプレート：

```python
"""
スクリプトの説明
"""

import pandas as pd
from pathlib import Path
import sys
import io

# UTF-8出力を強制（多言語文字の出力エラー防止）
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

def main():
    # CSVファイルパスをpathlibで指定
    csv_path = Path('output') / 'データ.csv'

    # UTF-8 BOMで読み込み
    df = pd.read_csv(csv_path, encoding='utf-8-sig')

    # 処理...

    # UTF-8 BOMで保存
    output_path = Path('output') / '出力.csv'
    df.to_csv(output_path, index=False, encoding='utf-8-sig')

    print(f"完了: {output_path}")

if __name__ == '__main__':
    main()
```

### 2. PyMuPDF使用時のテンプレート

```python
import fitz  # PyMuPDF
import pandas as pd
from pathlib import Path
import sys
import io

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

def extract_tables_from_pdf(pdf_path):
    """PDFから表を抽出"""
    doc = fitz.open(pdf_path)
    all_data = []

    for page_num in range(len(doc)):
        page = doc[page_num]

        # TableFinderからテーブルリストを取得
        table_finder = page.find_tables()
        tables = table_finder.tables

        for table_idx, table in enumerate(tables):
            table_data = table.extract()

            if not table_data or len(table_data) < 2:
                continue

            # ヘッダーを除く
            rows = table_data[1:]

            for row in rows:
                # ページ番号、表番号を追加
                row_with_meta = [page_num + 1, table_idx + 1] + list(row)
                all_data.append(row_with_meta)

    doc.close()

    # 最大列数に揃える
    if all_data:
        max_columns = max(len(row) for row in all_data)
        normalized_data = []
        for row in all_data:
            if len(row) < max_columns:
                normalized_row = list(row) + [''] * (max_columns - len(row))
            else:
                normalized_row = list(row)
            normalized_data.append(normalized_row)

        # 列名生成
        column_names = ['Page', 'Table'] + [f'Column_{i}' for i in range(max_columns - 2)]

        # DataFrame作成
        df = pd.DataFrame(normalized_data, columns=column_names)
        return df

    return None
```

### 3. エラーハンドリング

```python
import logging

# ロギング設定
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

try:
    # 処理
    df = pd.read_csv('file.csv', encoding='utf-8-sig')
except UnicodeDecodeError as e:
    logging.error(f"エンコーディングエラー: {e}")
except FileNotFoundError as e:
    logging.error(f"ファイルが見つかりません: {e}")
except Exception as e:
    logging.error(f"予期しないエラー: {e}")
    raise
```

---

## チェックリスト

新しいスクリプトを作成する際のチェックリスト：

- [ ] `sys.stdout`をUTF-8に設定
- [ ] `import sys, io`を追加
- [ ] ファイルパスは`pathlib.Path`を使用
- [ ] CSV読み込みは`encoding='utf-8-sig'`
- [ ] CSV書き込みは`encoding='utf-8-sig'`
- [ ] PyMuPDFは`table_finder.tables`でテーブルリスト取得
- [ ] DataFrameは列数を揃えてから作成
- [ ] エラーハンドリングを実装
- [ ] ログ出力を設定

---

## 関連ドキュメント

- `CLAUDE.md` - プロジェクト設定
- `for_claude/log.txt` - プロジェクトログ
- `for_claude/comparison_summary.csv` - pdfplumber vs PyMuPDF比較結果

---

## 更新履歴

- 2025-10-24: 初版作成
